name: Continuous Train & Deploy (SageMaker)

on:
  push:
    branches: ["main"]
    paths:
      - "src/**"
      - "data/**"
      - "requirements.txt"
      - ".github/workflows/continuous-train-deploy.yml"
  workflow_dispatch:
    inputs:
      endpoint_name:
        description: "SageMaker endpoint name to deploy/update"
        required: false
        type: string
      train_instance_type:
        description: "SageMaker training instance type"
        required: false
        type: string
      endpoint_instance_type:
        description: "SageMaker endpoint instance type"
        required: false
        type: string
      max_iter:
        description: "Training hyperparameter for Logistic Regression"
        required: false
        type: string

permissions:
  contents: read
  id-token: write

concurrency:
  group: train-deploy-${{ github.ref }}
  cancel-in-progress: true

jobs:
  train-deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      S3_BUCKET: ${{ secrets.S3_BUCKET }}

      # IMPORTANT: Put the secret into env so we can use env.* in `if:` (avoids workflow validation errors)
      AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}

      # Execution role for SageMaker jobs (must be assumable by SageMaker). If you don't have a separate one,
      # this falls back to AWS_ROLE_TO_ASSUME (not always correct, but keeping your original behavior).
      SAGEMAKER_ROLE_ARN: ${{ secrets.SAGEMAKER_ROLE_ARN || secrets.AWS_ROLE_TO_ASSUME }}

      ENDPOINT_NAME: ${{ github.event.inputs.endpoint_name || vars.ENDPOINT_NAME || 'iris-realtime-endpoint' }}
      TRAIN_INSTANCE_TYPE: ${{ github.event.inputs.train_instance_type || vars.TRAIN_INSTANCE_TYPE || 'ml.m5.large' }}
      ENDPOINT_INSTANCE_TYPE: ${{ github.event.inputs.endpoint_instance_type || vars.ENDPOINT_INSTANCE_TYPE || 'ml.t3.medium' }}
      MAX_ITER: ${{ github.event.inputs.max_iter || vars.MAX_ITER || '200' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure AWS credentials (OIDC)
        if: ${{ env.AWS_ROLE_TO_ASSUME != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure AWS credentials (access keys)
        if: ${{ env.AWS_ROLE_TO_ASSUME == '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload training data to S3
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os
          import boto3
          from pathlib import Path

          region = os.environ["AWS_REGION"]
          bucket = os.environ["S3_BUCKET"]
          sha = os.environ["GITHUB_SHA"]
          key = f"data/iris/{sha}/train.csv"

          s3 = boto3.client("s3", region_name=region)
          s3.upload_file(str(Path("data/iris.csv")), bucket, key)

          train_s3_uri = f"s3://{bucket}/{key}"
          print("Uploaded:", train_s3_uri)
          with open(os.environ["GITHUB_ENV"], "a", encoding="utf-8") as fh:
              fh.write(f"TRAIN_S3_URI={train_s3_uri}\n")
          PY

      - name: Train model on SageMaker
        id: train
        shell: bash
        run: |
          set -euo pipefail
          OUTPUT_S3_URI="s3://${S3_BUCKET}/models/iris/${GITHUB_SHA}/"
          echo "OUTPUT_S3_URI=${OUTPUT_S3_URI}" >> "${GITHUB_ENV}"

          if [ -z "${SAGEMAKER_ROLE_ARN}" ]; then
            echo "Missing SageMaker execution role ARN. Set secret SAGEMAKER_ROLE_ARN (or reuse AWS_ROLE_TO_ASSUME)."
            exit 1
          fi

          TRAIN_LOG="$(mktemp)"
          python src/sagemaker_jobs.py train \
            --role-arn "${SAGEMAKER_ROLE_ARN}" \
            --train-s3-uri "${TRAIN_S3_URI}" \
            --output-s3-uri "${OUTPUT_S3_URI}" \
            --instance-type "${TRAIN_INSTANCE_TYPE}" \
            --max-iter "${MAX_ITER}" \
            --region "${AWS_REGION}" \
            --job-name "iris-${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}" \
            | tee "${TRAIN_LOG}"

          TRAINING_JOB_NAME="$(grep -E 'Training job completed:' "${TRAIN_LOG}" | tail -n1 | sed 's/Training job completed: //')"
          if [ -z "${TRAINING_JOB_NAME}" ]; then
            echo "Could not parse training job name from logs"
            exit 1
          fi

          echo "training_job_name=${TRAINING_JOB_NAME}" >> "${GITHUB_OUTPUT}"

      - name: Deploy real-time endpoint
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "${{ steps.train.outputs.training_job_name }}" ]; then
            echo "Missing training job output"
            exit 1
          fi

          python src/sagemaker_jobs.py deploy \
            --role-arn "${SAGEMAKER_ROLE_ARN}" \
            --training-job-name "${{ steps.train.outputs.training_job_name }}" \
            --endpoint-name "${ENDPOINT_NAME}" \
            --instance-type "${ENDPOINT_INSTANCE_TYPE}" \
            --region "${AWS_REGION}"

      - name: Smoke test endpoint
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import boto3

          region = os.environ["AWS_REGION"]
          endpoint = os.environ["ENDPOINT_NAME"]
          client = boto3.client("sagemaker-runtime", region_name=region)

          payload = {"instances": [[5.1, 3.5, 1.4, 0.2]]}
          resp = client.invoke_endpoint(
              EndpointName=endpoint,
              ContentType="application/json",
              Accept="application/json",
              Body=json.dumps(payload).encode("utf-8"),
          )
          print(resp["Body"].read().decode("utf-8"))
          PY
